# Multimodal Embedding Explorer

This repository provides interactive visualization tools for exploring joint embeddings from multimodal models like CLIP, LLaVa, and GPT-4V using Datumaro and OTX.

## ğŸ“ Repository Structure

```plaintext
Multimodal-Embedding-Explorer/
â”‚â”€â”€ dataset/              # Sample dataset files
â”‚â”€â”€ docs/                 # Documentation and guides
â”‚â”€â”€ notebooks/            # Jupyter notebooks for experiments
â”‚â”€â”€ outputs/              # Model-generated outputs
â”‚â”€â”€ src/                  # Main source code
â”‚   â”œâ”€â”€ visualization/    # Interactive visualization tools (Streamlit, Plotly)
â”‚   â”œâ”€â”€ embeddings/       # Embedding computation scripts
â”‚   â”œâ”€â”€ preprocessing/    # Data preprocessing scripts
â”‚â”€â”€ tests/                # Unit tests
â”‚â”€â”€ LICENSE               # License information
â”‚â”€â”€ README.md             # Project description
â”‚â”€â”€ requirements.txt      # Python dependencies
â”‚â”€â”€ setup.py              # Installation script
```
 
An interactive visualization and annotation tool for exploring joint embedding spaces using open-source models like CLIP.  

## Features  
- Compute embeddings for images and text using CLIP.  
- Visualize embeddings in 3D space using Plotly.  
- Filter, zoom, and explore dataset relationships.  
- Annotate noisy/mislabeled data.  
- Perform fast similarity search with FAISS.  

## Installation  
```sh
git clone https://github.com/your-username/Multimodal-Embedding-Explorer.git
cd Multimodal-Embedding-Explorer
pip install -r requirements.txt
```

## Usage 
```python
src/visualization.py
```

